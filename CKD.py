# -*- coding: utf-8 -*-
"""CKD- final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19LJULbPMeagGBBvYzW2ZZxpx7YmdnyUA
"""

from google.colab import drive

drive.mount('/content/drive/')

"""# Importing the libraries"""

!pip install boruta

import pandas as pd
import seaborn as sp
import numpy as np
import random
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.impute import KNNImputer
from boruta import BorutaPy
import matplotlib.pyplot as plt

"""# Importing the dataset"""

ckd = pd.read_csv('/content/drive/MyDrive/Mini - Project/Dataset/kidney_disease.csv')
ckd.head()

ckd.info()

ckd

"""# DATA CLEANING"""

ckd['age'].fillna(ckd['age'].median(), inplace = True)
ckd['bp'].fillna(ckd['bp'].median(), inplace = True)
ckd['sc'].fillna(ckd['sc'].median(), inplace = True)
ckd['rbc'].fillna(ckd['rbc'].mode()[0], inplace = True)
ckd['pc'].fillna(ckd['pc'].mode()[0], inplace = True)
ckd['al'].fillna(ckd['al'].mode()[0], inplace = True)
ckd['su'].fillna(ckd['su'].mode()[0], inplace = True)
ckd['pcc'].fillna(ckd['pcc'].mode()[0], inplace = True)
ckd['htn'].fillna(ckd['htn'].mode()[0], inplace = True)
ckd['dm'].fillna(ckd['dm'].mode()[0], inplace = True)
ckd['cad'].fillna(ckd['cad'].mode()[0], inplace = True)
ckd['sg'].fillna(ckd['sg'].mode()[0], inplace = True)
ckd['ba'].fillna(ckd['ba'].mode()[0], inplace = True)
ckd['appet'].fillna(ckd['appet'].mode()[0], inplace = True)
ckd['pe'].fillna(ckd['pe'].mode()[0], inplace = True)
ckd['ane'].fillna(ckd['ane'].mode()[0], inplace = True)

ckd.info()

"""## KNN Imputation"""

# Select the column to impute
column_name = 'hemo'

# Create a KNN imputer
imputer = KNNImputer(n_neighbors=5)

# Perform KNN imputation
imputed_values = imputer.fit_transform(ckd[column_name].values.reshape(-1, 1))

# Assign imputed values back to the original dataset
ckd[column_name] = imputed_values.flatten()

# Print the updated dataset with imputed values
print(ckd)

ckd.info()

# Select the column to impute
column_name = 'bgr'

# Create a KNN imputer
imputer = KNNImputer(n_neighbors=5)

# Perform KNN imputation
imputed_values = imputer.fit_transform(ckd[column_name].values.reshape(-1, 1))

# Assign imputed values back to the original dataset
ckd[column_name] = imputed_values.flatten()

# Print the updated dataset with imputed values
print(ckd)

# Select the column to impute
column_name = 'bu'

# Create a KNN imputer
imputer = KNNImputer(n_neighbors=5)

# Perform KNN imputation
imputed_values = imputer.fit_transform(ckd[column_name].values.reshape(-1, 1))

# Assign imputed values back to the original dataset
ckd[column_name] = imputed_values.flatten()

# Print the updated dataset with imputed values
print(ckd)

ckd.info()

# Select the column to impute
column_name = 'sod'

# Create a KNN imputer
imputer = KNNImputer(n_neighbors=5)

# Perform KNN imputation
imputed_values = imputer.fit_transform(ckd[column_name].values.reshape(-1, 1))

# Assign imputed values back to the original dataset
ckd[column_name] = imputed_values.flatten()

# Print the updated dataset with imputed values
print(ckd)

# Select the column to impute
column_name = 'pot'

# Create a KNN imputer
imputer = KNNImputer(n_neighbors=5)

# Perform KNN imputation
imputed_values = imputer.fit_transform(ckd[column_name].values.reshape(-1, 1))

# Assign imputed values back to the original dataset
ckd[column_name] = imputed_values.flatten()

# Print the updated dataset with imputed values
print(ckd)

ckd.info()

"""## Convert non - numeric data to Nan"""

import numpy as np

# Assuming 'attribute' is the name of the attribute/column
attribute_values = ckd['rc']

numeric_values = pd.to_numeric(attribute_values, errors='coerce')

# Replace non-numeric values with NaN
ckd['rc'] = np.where(pd.to_numeric(attribute_values, errors='coerce').notnull(), attribute_values, np.nan)

# Select the column to impute
column_name = 'rc'

imputer = KNNImputer(n_neighbors=5)

imputed_values = imputer.fit_transform(ckd[column_name].values.reshape(-1, 1))

# Assign imputed values back to the original dataset
ckd[column_name] = imputed_values.flatten()

# Print the updated dataset with imputed values
print(ckd)

# Assuming 'attribute' is the name of the attribute/column
attribute_values = ckd['wc']

# Convert non-numeric values to NaN
numeric_values = pd.to_numeric(attribute_values, errors='coerce')

# Replace non-numeric values with NaN
ckd['wc'] = np.where(pd.to_numeric(attribute_values, errors='coerce').notnull(), attribute_values, np.nan)

# Select the column to impute
column_name = 'wc'

# Create a KNN imputer
imputer = KNNImputer(n_neighbors=5)

# Perform KNN imputation
imputed_values = imputer.fit_transform(ckd[column_name].values.reshape(-1, 1))

# Assign imputed values back to the original dataset
ckd[column_name] = imputed_values.flatten()

# Print the updated dataset with imputed values
print(ckd)

ckd.shape

attribute_values = ckd['pcv']

numeric_values = pd.to_numeric(attribute_values, errors='coerce')

# Replace non-numeric values with NaN
ckd['pcv'] = np.where(pd.to_numeric(attribute_values, errors='coerce').notnull(), attribute_values, np.nan)

# Select the column to impute
column_name = 'pcv'

# Create a KNN imputer
imputer = KNNImputer(n_neighbors=5)

# Perform KNN imputation
imputed_values = imputer.fit_transform(ckd[column_name].values.reshape(-1, 1))

# Assign imputed values back to the original dataset
ckd[column_name] = imputed_values.flatten()

# Print the updated dataset with imputed values
print(ckd)

ckd.head()

ckd.info()

"""# Class Distribution"""

ckd['classification'] = ckd['classification'].replace('ckd\t', 'ckd')

# Print the updated dataset
print(ckd)

#Class Distribution
class_counts_typ = ckd['classification'].value_counts()

class_distribution_typ =class_counts_typ / len(ckd) *100

print(class_distribution_typ)

#plot a bar graph
value = ckd['classification'].value_counts()

plt.bar(value.index, value.values)

plt.title('Classification')
plt.xlabel('Result')
plt.ylabel('Count')

plt.show()

"""# Data Transformation"""

# Select the features to be label encoded
features_to_encode = ['rbc', 'pc']

# Perform label encoding
label_encoder = LabelEncoder()
for feature in features_to_encode:
    ckd[feature] = label_encoder.fit_transform(ckd[feature])

# Print the transformed dataset
print(ckd[features_to_encode])

# Select the features to be label encoded
features_to_encode = ['pcc', 'ba']
# Perform label encoding
label_encoder = LabelEncoder()
for feature in features_to_encode:
    ckd[feature] = label_encoder.fit_transform(ckd[feature])

# Print the transformed dataset
print(ckd[features_to_encode])

# Select the features to be label encoded
features_to_encode = ['htn', 'dm','cad','appet','pe','ane','classification']
# Perform label encoding
label_encoder = LabelEncoder()
for feature in features_to_encode:
    ckd[feature] = label_encoder.fit_transform(ckd[feature])

# Print the transformed dataset
print(ckd[features_to_encode])

ckd.info()

"""# Normalization"""

## Outliers detection
import matplotlib.pyplot as plt
import numpy as np

# Assuming you have a pandas DataFrame named 'ckd' with the relevant features

# Create a box plot for each feature
plt.figure(figsize=(10, 6))
plt.boxplot([ckd['age'], ckd['bgr'], ckd['bu'], ckd['wc']], labels=['Age', 'BGR', 'BU', 'WC'])
plt.title('Box Plot of Features')
plt.ylabel('Values')
plt.xlabel('Features')
plt.show()

# Define a function to identify outliers based on the box plot
def identify_outliers(feature, feature_name):
    q1 = np.percentile(feature, 25)
    q3 = np.percentile(feature, 75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = (feature < lower_bound) | (feature > upper_bound)

    print('Outliers in {}:'.format(feature_name))
    print(np.where(outliers)[0])

# Identify outliers for each feature
identify_outliers(ckd['age'], 'Age')
identify_outliers(ckd['bgr'], 'BGR')
identify_outliers(ckd['bu'], 'BU')
identify_outliers(ckd['wc'], 'WC')

from sklearn.preprocessing import MinMaxScaler

# Assuming you have a pandas DataFrame named 'ckd' with the relevant attributes

# Select the attributes to normalize
attributes = ['age', 'bgr', 'bu', 'wc']

# Create a new DataFrame to store the normalized values
normalized_data = pd.DataFrame()

# Perform min-max normalization
scaler = MinMaxScaler()
normalized_data[attributes] = scaler.fit_transform(ckd[attributes])

# View the normalized data
print(normalized_data.head())

"""#Splitting the Dataset"""

X = ckd.drop('classification', axis=1)
y = ckd['classification']

print(X)

print(y)

"""# Data Balancing"""

smote = SMOTE()

X_resampled, y_resampled = smote.fit_resample(X, y)

resampled_data = X_resampled.copy()
resampled_data['classification'] = y_resampled

ckd = resampled_data

X_resampled.shape

y_resampled.shape

ckd.shape

ckd.head()

#Class Distribution
class_counts_typ = ckd['classification'].value_counts()

class_distribution_typ =class_counts_typ / len(ckd) *100

print(class_distribution_typ)

#plot a bar graph
value = ckd['classification'].value_counts()

plt.bar(value.index, value.values)

plt.title('Classification')
plt.xlabel('Result')
plt.ylabel('Count')

plt.show()

"""#Drop unnecessary feature"""

ckd.drop('id', axis = 1, inplace = True)

ckd.head()

"""# BORUTA"""

X = ckd.iloc[:, :-1].values
y = ckd.iloc[:, -1].values

# Define the random forest classifier
rf = RandomForestClassifier(n_estimators=100, n_jobs= -1)

# Define the Boruta feature selection method
boruta = BorutaPy(rf, n_estimators='auto', verbose=2)

# Perform feature selection
boruta.fit(X, y)

# Print the selected features
selected_features = ckd.columns[:-1][boruta.support_].tolist()
print("Selected features:", selected_features)

"""## K - NN"""

# Use consensus selected features with your classifier
x_selected = ckd[selected_features]

# Create the K-Nearest Neighbors classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Perform cross-validation with 10 folds
scores = cross_val_score(knn, x_selected, y, cv=10)

# Print the accuracy for each fold
for i, score in enumerate(scores, 1):
    print("Fold {}: Accuracy = {:.4f}".format(i, score))

# Calculate and print the mean accuracy and standard deviation of the cross-validation scores
mean_accuracy = scores.mean()

# Print the results
print("Accuracy for K-NN: %0.4f" % (mean_accuracy))

"""## Naive Bayesian"""

nb = GaussianNB()

# Perform cross-validation with 10 folds
scores = cross_val_score(nb, x_selected, y, cv=10)

# Print the accuracy for each fold
for i, score in enumerate(scores, 1):
    print("Fold {}: Accuracy = {:.4f}".format(i, score))

# Calculate and print the mean accuracy and standard deviation of the cross-validation scores
mean_accuracy = scores.mean()

# Print the results
print("Accuracy for Naive Bayesian: %0.4f" % (mean_accuracy))

"""## Logistic Regression"""

lr = LogisticRegression(max_iter = 5000, random_state=42)

# Perform cross-validation with 10 folds
scores = cross_val_score(lr, x_selected, y, cv=10)

# Print the accuracy for each fold
for i, score in enumerate(scores, 1):
    print("Fold {}: Accuracy = {:.4f}".format(i, score))

# Calculate and print the mean accuracy and standard deviation of the cross-validation scores
mean_accuracy = scores.mean()

# Print the results
print("Accuracy for Logistic Regression: %0.4f" % (mean_accuracy))

"""## Graph"""

model_names = ['KNN', 'Naive Bayesian', 'Logistic Regression']
accuracies = [0.8775, 0.9625, 0.9775]

colors = [(0.3, 0.5, 0.8, 0.7), (0.4, 0.7, 0.4, 0.7), (0.9, 0.6, 0.2, 0.7)]
plt.figure(figsize=(6, 4))

# Create the bar graph
plt.bar(model_names, accuracies, color = colors, width = 0.5)

# Add labels and title
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Boruta Feature Selection')

# Display the graph
plt.show()

"""# RANDOM FOREST FEATURE SELECTION"""

X = ckd.drop('classification', axis=1)
y = ckd['classification']

# Use random forest to select the most important features
rfc = RandomForestClassifier(n_estimators=100)
rfc.fit(X, y)
importance = rfc.feature_importances_
# Create a list of (feature name, importance) tuples and sort by importance
features = list(zip(X.columns, importance))
features.sort(key=lambda x: x[1], reverse=True)

# Print the sorted list of feature importances
for f in features:
    print(f)


# Select the top k features
k = 8
top_features = [f[0] for f in features[:k]]
print("Selected features: ")
for f in top_features:
    print(f)
X = X[top_features]

"""##K - NN"""

# Train and evaluate the model using KFold
kf = KFold(n_splits=10, shuffle=True, random_state=42)
accuracy_scores = []
y_preds=[]
y_tests=[]
for train_index, test_index in kf.split(X):
    # Split data into train and test sets for this fold
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Create and train the random forest classifier
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train, y_train)

    # Make predictions on the test set and calculate accuracy
    y_pred = knn.predict(X_test)
    y_preds.extend(y_pred)
    y_tests.extend(y_test)
    accuracy = accuracy_score(y_test, y_pred)

    # Add accuracy score to list
    accuracy_scores.append(accuracy)

# Compute and print the mean accuracy score and standard deviation
print("Accuracy- K-NN classifier: %0.4f (+/- %0.4f)" % (np.mean(accuracy_scores), np.std(accuracy_scores) * 2))

"""## Naive Bayesian"""

# Train and evaluate the model using KFold
kf = KFold(n_splits=10, shuffle=True, random_state=42)
accuracy_scores = []
y_preds=[]
y_tests=[]
for train_index, test_index in kf.split(X):
    # Split data into train and test sets for this fold
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    nb = GaussianNB()
    nb.fit(X_train, y_train)

    # Make predictions on the test set and calculate accuracy
    y_pred = nb.predict(X_test)
    y_preds.extend(y_pred)
    y_tests.extend(y_test)
    accuracy = accuracy_score(y_test, y_pred)

    # Add accuracy score to list
    accuracy_scores.append(accuracy)

# Compute and print the mean accuracy score and standard deviation
print("Accuracy Naive Bayesian classifier: %0.4f (+/- %0.2f)" % (np.mean(accuracy_scores), np.std(accuracy_scores) * 2))

"""## LOGISTIC REGRESSION"""

# Perform 10-fold cross validation using KFold method
kf = KFold(n_splits=10, shuffle=True, random_state=42)
lr = LogisticRegression(C=1, max_iter = 5000)
scores = []
y_preds = []
y_true = []
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    lr.fit(X_train, y_train)

    y_pred = lr.predict(X_test)
    y_preds.extend(y_pred)
    y_true.extend(y_test)
    scores.append(lr.score(X_test, y_test))


# Calculate and print the cross-validation accuracy
print("Accuracy- Logistic Regression: %0.4f (+/- %0.2f)" % (np.mean(scores), np.std(scores) * 2))

"""##Graph"""

model_names = ['KNN', 'Naive Bayesian', 'Logistic Regression']
accuracies = [0.9475, 0.9375, 0.9750]

colors = [(0.3, 0.5, 0.8, 0.7), (0.4, 0.7, 0.4, 0.7), (0.9, 0.6, 0.2, 0.7)]
plt.figure(figsize=(6, 4))

# Create the bar graph
plt.bar(model_names, accuracies, color = colors, width=0.5)

# Add labels and title
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Random Forest Feature Selection')

# Display the graph
plt.show()